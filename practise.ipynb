{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization:\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', 'i', 'would', 'like', 'to', 'speek', 'to', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Simple: Tokenize a given text into words.\n",
    "tokenized_Word = word_tokenize('Hello there i would like to speek to you')\n",
    "print(tokenized_Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there i would like to speek to you']\n"
     ]
    }
   ],
   "source": [
    "# Intermediate: Tokenize a text into sentences.\n",
    "tokenized_sentence = sent_tokenize('Hello there i would like to speek to you')\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'her', 'dog', \"'s\", 'name', 'is', 'malia', 'i', 'really', 'love', 'her', 'so', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Complex: Tokenize a text, remove punctuation, and convert words to lowercase.\n",
    "import string\n",
    "\n",
    "\n",
    "text=\"\"\"Hello,her dog's name is, Malia. I really love her so much!\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_remove_punction(text):\n",
    "    tokenized_Words=word_tokenize(text)\n",
    "    \n",
    "    clean_words = [word.lower() for word in tokenized_Words if word not in string.punctuation]\n",
    "    return(clean_words)\n",
    "\n",
    "clean_words = tokenize_remove_punction(text)\n",
    "print(clean_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-of-Speech Tagging:\n",
    "from nltk import pos_tag, word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tommorrow', 'NN'), ('i', 'NN'), ('will', 'MD'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('grand', 'JJ'), ('canyon', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Simple: Tag the parts of speech for words in a given sentence.\n",
    "text='Tommorrow i will go to the grand canyon'\n",
    "tokenized_Words = word_tokenize(text)\n",
    "pos_tagged_words = pos_tag(tokenized_Words)\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), ('computer', 'NN'), ('science', 'NN'), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('particular', 'JJ'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Intermediate: Tag the parts of speech for words in a paragraph or longer text.\n",
    "\n",
    "paragraph = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, \n",
    "computer science, and artificial intelligence concerned with \n",
    "the interactions between computers and human language, in particular \n",
    "how to program computers to process and analyze large amounts of natural \n",
    "language data.\n",
    "\"\"\"\n",
    "\n",
    "tokenized_paragraph = sent_tokenize(paragraph)\n",
    "no_punt=[]\n",
    "for sentences in tokenized_paragraph:\n",
    "    words = word_tokenize(sentences)\n",
    "    for word in words:\n",
    "        if word not in string.punctuation:\n",
    "          no_punt.append(word)  \n",
    "   \n",
    "tagged_words = pos_tag(no_punt)\n",
    "\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('omg', 'NN'), ('I', 'PRP'), ('ca', 'MD'), (\"n't\", 'RB'), ('believe', 'VB'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('already', 'RB'), ('#', '#'), ('friday', 'JJ'), ('üòÉ', 'NN')]\n",
      "[('just', 'RB'), ('saw', 'VBD'), ('the', 'DT'), ('cutest', 'JJS'), ('puppy', 'JJ'), ('ever', 'RB'), ('üê∂', 'RB'), ('#', '#'), ('puppylove', 'VB')]\n",
      "[('so', 'RB'), ('tired', 'JJ'), ('üò¥', 'NN')]\n",
      "[('LOL', 'NNP'), ('that', 'DT'), ('meme', 'NN'), ('is', 'VBZ'), ('hilarious', 'JJ'), ('üòÇ', 'NNS')]\n",
      "[('ca', 'MD'), (\"n't\", 'RB'), ('wait', 'VB'), ('for', 'IN'), ('the', 'DT'), ('weekend', 'NN'), ('!', '.'), ('üéâ', 'NN')]\n",
      "[('ugh', 'JJ'), ('Mondays', 'NNS'), ('are', 'VBP'), ('the', 'DT'), ('worst', 'JJS'), ('üò©', 'NN')]\n",
      "[('watching', 'VBG'), ('a', 'DT'), ('scary', 'JJ'), ('movie', 'NN'), ('alone', 'RB'), ('at', 'IN'), ('night', 'NN'), ('üëª', 'NN')]\n",
      "[('this', 'DT'), ('weather', 'NN'), ('is', 'VBZ'), ('amazing', 'JJ'), ('‚òÄÔ∏è', 'NN')]\n",
      "[('new', 'JJ'), ('phone', 'NN'), ('who', 'WP'), ('dis', 'VBZ'), ('?', '.'), ('üì±', 'NN')]\n",
      "[('feeling', 'VBG'), ('blessed', 'VBN'), ('üôè', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Complex: Perform part-of-speech tagging on a dataset of tweets or short texts with informal language and slang.\n",
    "\n",
    "tweets = [\n",
    "    \"omg I can't believe it's already #friday üòÉ\",\n",
    "    \"just saw the cutest puppy ever üê∂ #puppylove\",\n",
    "    \"so tired üò¥\",\n",
    "    \"LOL that meme is hilarious üòÇ\",\n",
    "    \"can't wait for the weekend! üéâ\",\n",
    "    \"ugh Mondays are the worst üò©\",\n",
    "    \"watching a scary movie alone at night üëª\",\n",
    "    \"this weather is amazing ‚òÄÔ∏è\",\n",
    "    \"new phone who dis? üì±\",\n",
    "    \"feeling blessed üôè\"\n",
    "]\n",
    "\n",
    "# Perform part-of-speech tagging for each tweet\n",
    "for tweet in tweets:\n",
    "    # Tokenize the tweet into words\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # Print the tagged words for the tweet\n",
    "    print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
