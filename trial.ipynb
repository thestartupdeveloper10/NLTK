{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a835aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b640902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus : Body of text, singular. Corpora is the plural of this. \n",
    "#Lexicon : Words and their meanings. \n",
    "#Token : Each “entity” that is a part of whatever was split up based on rules.\n",
    "#In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f7c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ibande/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512cdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Today, we revel in the sweet taste of victory in the annual Nam Oyiech tournament. Our success wasn't about reinventing the wheel; it was about being strategists, designers, and developers—small enough to be quick, big enough to deliver excellence.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1c89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'we', 'revel', 'in', 'the', 'sweet', 'taste', 'of', 'victory', 'in', 'the', 'annual', 'Nam', 'Oyiech', 'tournament', '.', 'Our', 'success', 'was', \"n't\", 'about', 'reinventing', 'the', 'wheel', ';', 'it', 'was', 'about', 'being', 'strategists', ',', 'designers', ',', 'and', 'developers—small', 'enough', 'to', 'be', 'quick', ',', 'big', 'enough', 'to', 'deliver', 'excellence', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize words\n",
    "#separate each word and puntuation\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a4ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today, we revel in the sweet taste of victory in the annual Nam Oyiech tournament.', \"Our success wasn't about reinventing the wheel; it was about being strategists, designers, and developers—small enough to be quick, big enough to deliver excellence.\"]\n"
     ]
    }
   ],
   "source": [
    "#sent_tokenize\n",
    "#separate paragraphs\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b994479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 4), ('the', 3), ('in', 2)]\n"
     ]
    }
   ],
   "source": [
    "#frequency distribution\n",
    "#the most_common () displayed\n",
    "fd = FreqDist(tokenized_word)\n",
    "print(fd.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f96d951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ibande/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d32483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fbe3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'revel', 'sweet', 'taste', 'victory', 'annual', 'Nam', 'Oyiech', 'tournament', '.', 'Our', 'success', \"n't\", 'reinventing', 'wheel', ';', 'strategists', ',', 'designers', ',', 'developers—small', 'enough', 'quick', ',', 'big', 'enough', 'deliver', 'excellence', '.']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords from tokenized_word\n",
    "tokenized_word_without_stop_words = []\n",
    "\n",
    "for word in tokenized_word:\n",
    "    if word not in stop_words:\n",
    "        tokenized_word_without_stop_words.append(word)\n",
    "print(tokenized_word_without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c482d77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'in', 'was', 'the', 'be', 'it', 'being', 'about', 'we', 'of', 'to'}\n",
      "47\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "#see the difference\n",
    "print(set(tokenized_word)-set(tokenized_word_without_stop_words))\n",
    "print(len(tokenized_word))\n",
    "print(len(tokenized_word_without_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b1be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ibande/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment analysis,lemmatization and stemming\n",
    "#converts word to their original form\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a627f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After after After\n",
      "experiencing experienc experiencing\n",
      "sadness sad sadness\n",
      "and and and\n",
      "disappointment disappoint disappointment\n",
      ", , ,\n",
      "she she she\n",
      "found found found\n",
      "joy joy joy\n",
      "and and and\n",
      "excitement excit excitement\n",
      "in in in\n",
      "the the the\n",
      "peaceful peac peaceful\n",
      "moments moment moment\n",
      ", , ,\n",
      "feeling feel feeling\n",
      "grateful grate grateful\n",
      "and and and\n",
      "thrilled thrill thrilled\n",
      "despite despit despite\n",
      "her her her\n",
      "initial initi initial\n",
      "fear fear fear\n",
      "and and and\n",
      "anxiety anxieti anxiety\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "# Stemming is a crude heuristic process that chops off the ends of words to obtain their root forms, known as stems.The resulting stem \n",
    "# may not always be a valid word in the language but is intended to represent the core meaning shared by related words. \n",
    "\n",
    "# Lemmatization, on the other hand, uses a more sophisticated approach based on vocabulary and morphological analysis of words.\n",
    "# It reduces words to their canonical or dictionary forms, known as lemmas, which are actual words found in the dictionary\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "demoWords = \"After experiencing sadness and disappointment, she found joy and excitement in the peaceful moments, feeling grateful and thrilled despite her initial fear and anxiety.\"\n",
    "tokenized_words = word_tokenize(demoWords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in tokenized_words:\n",
    "    print(word, stemmer.stem(word), lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32be6745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ibande/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentment analysis\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc3a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "#we use polarity analysis to show negative,neutral or positive sentiment\n",
    "print(sia.polarity_scores(\"i love food\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e599d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any substance that can be metabolized by an animal to give energy and build tissue\n"
     ]
    }
   ],
   "source": [
    "#find meaning,synonyms and antonyms\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = wordnet.synset('food.n.01')  # 'leg.n.01' is the unique identifier for the first noun sense of 'leg'\n",
    "print(word.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cf3d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generous', 'generous', 'generous']\n"
     ]
    }
   ],
   "source": [
    "#synonym\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('generous'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eb72499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wake']\n"
     ]
    }
   ],
   "source": [
    "#antonyms\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets('sleep'):\n",
    "    for lemma in syn.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "            antonyms.append(antonym.name())\n",
    "print(antonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77ba844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ibande/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9942f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Define a sentence\n",
    "a_sentence = \"NLTK is a powerful tool for natural language processing.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(a_sentence)\n",
    "\n",
    "# Perform part-of-speech tagging on the tokenized words\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the result\n",
    "print(pos_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0cf1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a23584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  44th/JJ\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER): \n",
    "# Identifying and classifying named entities in text into pre-defined categories like person names,\n",
    "# organizations, locations, etc.\n",
    "\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "words = word_tokenize(\"Barack Obama was the 44th President of the United States.\")\n",
    "ner_tags = ne_chunk(pos_tag(words))\n",
    "print(ner_tags)\n",
    "\n",
    "# Geo-Political Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1783671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.3347122780719073\n",
      "  (0, 1)\t0.3347122780719073\n",
      "  (0, 2)\t0.3347122780719073\n",
      "  (0, 6)\t0.4704264280854632\n",
      "  (0, 4)\t0.4704264280854632\n",
      "  (0, 3)\t0.4704264280854632\n",
      "  (1, 7)\t0.5330978245262535\n",
      "  (1, 0)\t0.5330978245262535\n",
      "  (1, 5)\t0.3793034928087496\n",
      "  (1, 1)\t0.3793034928087496\n",
      "  (1, 2)\t0.3793034928087496\n",
      "[[0.38087261]]\n"
     ]
    }
   ],
   "source": [
    "# Text Similarity: Measuring how similar two texts are to each other.\n",
    "\n",
    "#Example (using cosine similarity):\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "documents = [\"NLTK is a powerful tool for natural language processing.\",\n",
    "             \"Natural language processing is the future of the world.\"]\n",
    "\n",
    "# Retrieve English stopwords as a string\n",
    "stop_words = 'english'\n",
    "\n",
    "\n",
    "# Term frequency inverse Document frequency\n",
    "# The TfidfVectorizer internally handles the stopwords removal based on the language specified in the stop_words parameter.\n",
    "# In this case, setting stop_words='english' instructs the vectorizer to use NLTK's list of English\n",
    "# stopwords to remove common English words from the text data during the TF-IDF vectorization process.\n",
    "\n",
    "\n",
    "# Initialize TF-IDF vectorizer with English stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Convert Documents to TF-IDF Matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Calculate Similarity\n",
    "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fa9417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely' 'acting' 'amazing' 'and' 'asleep' 'away' 'best' 'blown'\n",
      " 'bother' 'breathtaking' 'by' 'captivating' 'cinematography' 'confusing'\n",
      " 'couldn' 'didn' 'don' 'ever' 'everyone' 'fell' 'film' 'finish' 'for'\n",
      " 'from' 'garbage' 'great' 'halfway' 'highly' 'is' 'it' 'like' 'loved'\n",
      " 'money' 'movie' 'movies' 'must' 'my' 'not' 'of' 'on' 'one' 'performances'\n",
      " 'plot' 'price' 'recommend' 'seen' 'stand' 'start' 'storyline' 'superb'\n",
      " 'terrible' 'the' 'this' 'through' 'ticket' 'to' 've' 'was' 'wasted'\n",
      " 'watch' 'watching' 'worst' 'worth' 'year']\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Text Classification: Assigning predefined categories or labels to text documents.\n",
    "\n",
    "#Example (using Naive Bayes Classifier):\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#  convert text data into a numerical representation\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Represents the Naive Bayes classifier we'll use for text classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split our dataset into training and testing sets\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# calculate the accuracy of our classifier.\n",
    "\n",
    "reviews = [\n",
    "    (\"This movie is amazing!\", \"positive\"),\n",
    "    (\"I didn't like this movie.\", \"negative\"),\n",
    "    (\"The acting was superb.\", \"positive\"),\n",
    "    (\"The plot was confusing.\", \"negative\"),\n",
    "    (\"I absolutely loved it!\", \"positive\"),\n",
    "    (\"The worst movie I've ever seen.\", \"negative\"),\n",
    "    (\"The cinematography was breathtaking.\", \"positive\"),\n",
    "    (\"I couldn't stand watching it.\", \"negative\"),\n",
    "    (\"A must-watch for everyone!\", \"positive\"),\n",
    "    (\"Terrible acting and terrible plot.\", \"negative\"),\n",
    "    (\"I was blown away by this film.\", \"positive\"),\n",
    "    (\"I wasted my money on this garbage.\", \"negative\"),\n",
    "    (\"The best movie of the year!\", \"positive\"),\n",
    "    (\"I fell asleep halfway through.\", \"negative\"),\n",
    "    (\"Great storyline and great performances.\", \"positive\"),\n",
    "    (\"Don't bother watching it.\", \"negative\"),\n",
    "    (\"One of the worst movies I've ever seen.\", \"negative\"),\n",
    "    (\"Captivating from start to finish.\", \"positive\"),\n",
    "    (\"Not worth the ticket price.\", \"negative\"),\n",
    "    (\"Highly recommend it to everyone!\", \"positive\")\n",
    "]\n",
    "\n",
    "#  Extract the review texts (features) from the reviews dataset.\n",
    "X = [review[0] for review in reviews]\n",
    "\n",
    "# Extract the sentiment labels (targets) from the reviews dataset.\n",
    "y = [review[1] for review in reviews]\n",
    "\n",
    "\n",
    "#  Initialize a CountVectorizer object, convert the review texts into a numerical format suitable for the classifier.\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# convert the raw text data into a sparse matrix of token counts.\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_vectorized.toarray())\n",
    "# We allocate 20% of the data for testing and 80% for training. \n",
    "# The random_state parameter ensures reproducibility by fixing the random seed.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the model\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier using the training data (features X_train and labels y_train) using the fit method.\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier by comparing the predicted labels (predictions)\n",
    "# with the true labels (y_test) using the accuracy_score function.\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0703bdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"This is the first document.\",\n",
    "             \"This document is the second document.\",\n",
    "             \"And this is the third one.\",\n",
    "             \"Is this the first document?\"]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into a matrix of token counts\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the matrix of token counts\n",
    "print(X.toarray())\n",
    "print(X)\n",
    "\n",
    "\n",
    "# The numbers in the matrix represent the frequency of each word in the corresponding document.\n",
    "# For example, the value 2 in row 1, column 1 indicates that the word \"document\" appears twice in the second document.\n",
    "# In the first document, the word \"and\" doesn't appear, \"document\", \"first\", \"is\", \"the\", and \"this\" each appear once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3e392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
